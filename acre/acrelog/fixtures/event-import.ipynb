{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a how to guide for importing data from .csv file to Django database (SQLite). You will see example data classes. Please note that those classes are for demonstration only. You are free to create classes in the way you see fit the best. \n",
    "\n",
    "The end goal is to create a JSON data that will be used to import data by Django built-in command. The format we want is structured as below. (more detail check -> https://docs.djangoproject.com/en/5.1/howto/initial-data/)\n",
    "\n",
    "```JSON\n",
    "[\n",
    "  {\n",
    "    \"model\": \"myapp.classname\",\n",
    "    \"pk\": 1,\n",
    "    \"fields\": {\n",
    "      \"attribute_name1\": \"Value\",\n",
    "      \"attribute_name2\": \"Value\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"model\": \"myapp.field\",\n",
    "    \"pk\": 2,\n",
    "    \"fields\": {\n",
    "      \"field_id\": \"32\",\n",
    "      \"field_name\": \"Corner\"\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "In Python, you can assume that JSON is Dictionary-like data type. So, the overall step is .csv -> Pandas dataframe -> dictionary ->JSON. \n",
    "\n",
    "Let's start with importing necessary modules. In this case, we only need 2 modules. Then load data from .csv to Pandas dataframe (we did this before in lab 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python's module\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>operator</th>\n",
       "      <th>location</th>\n",
       "      <th>operation</th>\n",
       "      <th>note</th>\n",
       "      <th>Power Unit</th>\n",
       "      <th>Seeds planted</th>\n",
       "      <th>Seeding Rate (seeds/ac)</th>\n",
       "      <th>Fertilizers applied</th>\n",
       "      <th>Fertilizer Rate (lb/ac)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4/24/2022</td>\n",
       "      <td>Evan</td>\n",
       "      <td>200</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>2-4D round up burn down</td>\n",
       "      <td>Hagie STS12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4/27/2022</td>\n",
       "      <td>Bryan</td>\n",
       "      <td>6</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>20oz ru, 11 24d</td>\n",
       "      <td>Hagie STS12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4/27/2022</td>\n",
       "      <td>Bryan</td>\n",
       "      <td>5</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>20 oz ru/ 11 oz 24d</td>\n",
       "      <td>Hagie STS12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4/27/2022</td>\n",
       "      <td>Bryan</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>20 oz ru. 11 oz 24d</td>\n",
       "      <td>Hagie STS12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4/27/2022</td>\n",
       "      <td>Bryan</td>\n",
       "      <td>101</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>20 oz ru 11 oz 24d</td>\n",
       "      <td>Hagie STS12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>11/17/2022</td>\n",
       "      <td>Ceres</td>\n",
       "      <td>105</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>VRF MAP: 153 lbs/ac\\r\\nVRF Potash: 133 lbs/ac</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAP,Potash (0-0-60)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>11/17/2022</td>\n",
       "      <td>Ceres</td>\n",
       "      <td>111</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>VRF MAP: 139 lbs/ac\\r\\nVRF Potash: 121 lbs/ac</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAP,Potash (0-0-60)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>11/17/2022</td>\n",
       "      <td>Ceres</td>\n",
       "      <td>33</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>VRF MAP: 154 lbs/ac</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAP</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>11/17/2022</td>\n",
       "      <td>Ceres</td>\n",
       "      <td>41</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>VRF MAP: 115.8 lbs/ac</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAP</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>11/17/2022</td>\n",
       "      <td>Ceres</td>\n",
       "      <td>5</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>MAP: 240 lbs/ac\\r\\nPotash: 225 lbs/ac</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAP,Potash (0-0-60)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date operator location     operation  \\\n",
       "0     4/24/2022     Evan      200  Spread/Spray   \n",
       "1     4/27/2022    Bryan        6  Spread/Spray   \n",
       "2     4/27/2022    Bryan        5  Spread/Spray   \n",
       "3     4/27/2022    Bryan   Cotton  Spread/Spray   \n",
       "4     4/27/2022    Bryan      101  Spread/Spray   \n",
       "..          ...      ...      ...           ...   \n",
       "428  11/17/2022    Ceres      105  Spread/Spray   \n",
       "429  11/17/2022    Ceres      111  Spread/Spray   \n",
       "430  11/17/2022    Ceres       33  Spread/Spray   \n",
       "431  11/17/2022    Ceres       41  Spread/Spray   \n",
       "432  11/17/2022    Ceres        5  Spread/Spray   \n",
       "\n",
       "                                              note   Power Unit Seeds planted  \\\n",
       "0                         2-4D round up burn down   Hagie STS12           NaN   \n",
       "1                                  20oz ru, 11 24d  Hagie STS12           NaN   \n",
       "2                              20 oz ru/ 11 oz 24d  Hagie STS12           NaN   \n",
       "3                              20 oz ru. 11 oz 24d  Hagie STS12           NaN   \n",
       "4                               20 oz ru 11 oz 24d  Hagie STS12           NaN   \n",
       "..                                             ...          ...           ...   \n",
       "428  VRF MAP: 153 lbs/ac\\r\\nVRF Potash: 133 lbs/ac          NaN           NaN   \n",
       "429  VRF MAP: 139 lbs/ac\\r\\nVRF Potash: 121 lbs/ac          NaN           NaN   \n",
       "430                            VRF MAP: 154 lbs/ac          NaN           NaN   \n",
       "431                          VRF MAP: 115.8 lbs/ac          NaN           NaN   \n",
       "432          MAP: 240 lbs/ac\\r\\nPotash: 225 lbs/ac          NaN           NaN   \n",
       "\n",
       "     Seeding Rate (seeds/ac)  Fertilizers applied  Fertilizer Rate (lb/ac)  \n",
       "0                        NaN                  NaN                      NaN  \n",
       "1                        NaN                  NaN                      NaN  \n",
       "2                        NaN                  NaN                      NaN  \n",
       "3                        NaN                  NaN                      NaN  \n",
       "4                        NaN                  NaN                      NaN  \n",
       "..                       ...                  ...                      ...  \n",
       "428                      NaN  MAP,Potash (0-0-60)                      NaN  \n",
       "429                      NaN  MAP,Potash (0-0-60)                      NaN  \n",
       "430                      NaN                  MAP                      NaN  \n",
       "431                      NaN                  MAP                      NaN  \n",
       "432                      NaN  MAP,Potash (0-0-60)                      NaN  \n",
       "\n",
       "[433 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "log_df = pd.read_csv(\"operation-log.csv\")  \n",
    "log_df\n",
    "# _df stands for data frame. It is a common suffix to indicate variable type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First data class I want to create is event class ass below. As you can see that I created a class with attributes that we don't have data. That is okay, we can leave those attributes optional by adding `blank=True, null=True` (more detail here -> https://docs.djangoproject.com/en/5.1/ref/models/fields/#field-options).\n",
    "\n",
    "```python \n",
    "class event(models.Model):\n",
    "    # event_date = models.DateTimeField(default=datetime.now, blank=True)\n",
    "    event_operator = models.CharField(max_length=50)\n",
    "    event_operation = models.CharField(choices=EVENT_TYPES, max_length=10)\n",
    "    event_operator = models.CharField(max_length=30)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.event_id\n",
    "```\n",
    "\n",
    "You can think of a database class as a table. So, let's create an `event` table (dataframe). We will use Pandas' Unique fucntion to see how many event rows we need to create. (more detail about unique function -> https://pandas.pydata.org/docs/reference/api/pandas.unique.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Spread/Spray', 'Tillage', 'Plant', 'Harvest', 'Soil Sampled'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! check this DK\n",
    "log_df['operation'].unique() # find a unique value in dataframe's `event` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from unique function is an array which is iterable. We will iterate over the array and create a dataframe. There are several ways to create a dataframe. But I recommend doing by using a list of dict or a dict of list. I will show you the first case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'operation': 'Spread/Spray'},\n",
       " {'operation': 'Tillage'},\n",
       " {'operation': 'Plant'},\n",
       " {'operation': 'Harvest'},\n",
       " {'operation': 'Soil Sampled'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list which we will add dictionaries later\n",
    "event = [] # suffix s is common way to indicate a list (array)\n",
    "\n",
    "# ! check this DK\n",
    "for operation in log_df['operation'].unique(): # we iterate over the result from unique function\n",
    "    event.append( # append new element to the list\n",
    "        { # curly bracket indicate the starting of dictionary\n",
    "            \"operation\": operation, # each key-value pair\n",
    "        } # the end of dictionary\n",
    "    )\n",
    "event # check the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of dictionaries. Making a dataframe is just one function. Note that I leave `lastname` and `phone` emtyp. This is an explicit way to work with optional attribute. You can also just ignore that attribute like I did with `middlename`. You will get the same outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spread/Spray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tillage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Plant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harvest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Soil Sampled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      operation\n",
       "0  Spread/Spray\n",
       "1       Tillage\n",
       "2         Plant\n",
       "3       Harvest\n",
       "4  Soil Sampled"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe\n",
    "event_df = pd.DataFrame(event)\n",
    "event_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to structure the data as required (check data format above). We will need to add `model` which is the class's name in the database. And `pk` or primary key which is the row's index. We want JSON at the end, but JSON is pretty much like dictionary in Python. So, we just need to create a (list of) dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'acrelog.operation',\n",
       "  'pk': 1,\n",
       "  'fields': {'operation': 'Spread/Spray'}},\n",
       " {'model': 'acrelog.operation', 'pk': 2, 'fields': {'operation': 'Tillage'}},\n",
       " {'model': 'acrelog.operation', 'pk': 3, 'fields': {'operation': 'Plant'}},\n",
       " {'model': 'acrelog.operation', 'pk': 4, 'fields': {'operation': 'Harvest'}},\n",
       " {'model': 'acrelog.operation',\n",
       "  'pk': 5,\n",
       "  'fields': {'operation': 'Soil Sampled'}}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = [] # create an empty list\n",
    "for index, row in event_df.iterrows(): # we iterate over the dataframe row by row\n",
    "    events.append( # append new element to the list\n",
    "        { # curly bracket indicate the starting of dictionary\n",
    "            \"model\": \"acrelog.operation\",  # the class name\n",
    "            \"pk\": index+1, # index starts at 0 while primary key starts at 1. \n",
    "            \"fields\": { # this is a dictionary of attributes\n",
    "                \"operation\": row['operation'] # only need firstname for now\n",
    "            }\n",
    "        } # the end of dictionary\n",
    "    )\n",
    "events # see the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data in the format that we need. To convert into JSON, you can just run the following command. Then you can copy the result and check with https://jsonlint.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"model\": \"acrelog.operation\", \"pk\": 1, \"fields\": {\"operation\": \"Spread/Spray\"}}, {\"model\": \"acrelog.operation\", \"pk\": 2, \"fields\": {\"operation\": \"Tillage\"}}, {\"model\": \"acrelog.operation\", \"pk\": 3, \"fields\": {\"operation\": \"Plant\"}}, {\"model\": \"acrelog.operation\", \"pk\": 4, \"fields\": {\"operation\": \"Harvest\"}}, {\"model\": \"acrelog.operation\", \"pk\": 5, \"fields\": {\"operation\": \"Soil Sampled\"}}]'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(events)\n",
    "# or write the JSON to the file directly \n",
    "with open('operation-data.json', 'w') as fp:\n",
    "    json.dump(events, fp=fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder why we create a dictionary then convert into a dataframe just for turn it into a dictionary again. Can you just make a dictionary that follows the required stucture directly? Absolutely yes. In some cases, that will be the better way to deal with the data. But there is a usecase for a dataframe as well. You will see it soon. For now, let's create another data class. \n",
    "\n",
    "```python\n",
    "class Operation(models.Model):\n",
    "    date = models.DateField()\n",
    "    note = models.CharField(max_length=300, blank=True, null=True)\n",
    "    event = models.ForeignKey(event, on_delete=models.CASCADE)\n",
    "```\n",
    "In this class, it has a foreignkey that points to event class. So, instead of recording event name, we will keep the primary key that points to that event. For better understanding, I will trim the original dataframe to keep only the data that we will play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>operation</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4/24/2022</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>2-4D round up burn down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4/27/2022</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>20oz ru, 11 24d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4/27/2022</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>20 oz ru/ 11 oz 24d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4/27/2022</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>20 oz ru. 11 oz 24d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4/27/2022</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>20 oz ru 11 oz 24d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>11/17/2022</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>VRF MAP: 153 lbs/ac\\r\\nVRF Potash: 133 lbs/ac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>11/17/2022</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>VRF MAP: 139 lbs/ac\\r\\nVRF Potash: 121 lbs/ac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>11/17/2022</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>VRF MAP: 154 lbs/ac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>11/17/2022</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>VRF MAP: 115.8 lbs/ac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>11/17/2022</td>\n",
       "      <td>Spread/Spray</td>\n",
       "      <td>MAP: 240 lbs/ac\\r\\nPotash: 225 lbs/ac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date     operation                                           note\n",
       "0     4/24/2022  Spread/Spray                       2-4D round up burn down \n",
       "1     4/27/2022  Spread/Spray                                20oz ru, 11 24d\n",
       "2     4/27/2022  Spread/Spray                            20 oz ru/ 11 oz 24d\n",
       "3     4/27/2022  Spread/Spray                            20 oz ru. 11 oz 24d\n",
       "4     4/27/2022  Spread/Spray                             20 oz ru 11 oz 24d\n",
       "..          ...           ...                                            ...\n",
       "428  11/17/2022  Spread/Spray  VRF MAP: 153 lbs/ac\\r\\nVRF Potash: 133 lbs/ac\n",
       "429  11/17/2022  Spread/Spray  VRF MAP: 139 lbs/ac\\r\\nVRF Potash: 121 lbs/ac\n",
       "430  11/17/2022  Spread/Spray                            VRF MAP: 154 lbs/ac\n",
       "431  11/17/2022  Spread/Spray                          VRF MAP: 115.8 lbs/ac\n",
       "432  11/17/2022  Spread/Spray          MAP: 240 lbs/ac\\r\\nPotash: 225 lbs/ac\n",
       "\n",
       "[433 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can subset the dataframe with any columns (and order)\n",
    "sub_log_df = log_df[['date', 'operation', 'note']]\n",
    "sub_log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the map function (more detail -> https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html) to map from the event's name to primary key. The map functions take several options. But we will use dictionary. The dictionary we want has event's names as keys and primary keys as values. Do you feel that this structure looks familar. Yes, that is the event dataframe that we created earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Spread/Spray': 1, 'Tillage': 2, 'Plant': 3, 'Harvest': 4, 'Soil Sampled': 5}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'operation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\dsyan\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'operation'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# add a new column 'Event#' to the event_df\u001b[39;00m\n\u001b[0;32m      9\u001b[0m sub_log_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent#\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sub_log_df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 10\u001b[0m sub_log_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moperation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msub_log_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moperation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmap(name2pk) \n\u001b[0;32m     11\u001b[0m pd\u001b[38;5;241m.\u001b[39mto_datetime(sub_log_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m sub_log_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(sub_log_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dsyan\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\dsyan\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'operation'"
     ]
    }
   ],
   "source": [
    "event_df\n",
    "event_df.to_dict()\n",
    "event_df.to_dict()['operation']\n",
    "index_name = event_df.to_dict()['operation']\n",
    "name2pk = dict((v, k+1) for k, v in index_name.items())\n",
    "print(name2pk)\n",
    "\n",
    "# add a new column 'Event#' to the event_df\n",
    "sub_log_df['Event#'] = sub_log_df.index + 1\n",
    "sub_log_df['operation'] = sub_log_df['operation'].map(name2pk) \n",
    "pd.to_datetime(sub_log_df['date'])\n",
    "sub_log_df['date'] = pd.to_datetime(sub_log_df['date']).dt.strftime('%Y-%m-%d')\n",
    "sub_log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned earlier, dataframe is made from a list of dictionaries or a dictionary of lists. We can convert it back as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format you see above is a dictionary of list. We want to use firstname to map with primary key (or dataframe's index). So we will only care about the `firstname`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost done. We want a dictionary that keys are firstname and values are primary keys. But what we got is opposite. No problem, we just need to inverse it (and add 1 to pandas' index to make it become primary key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got it. Now, let map the event's name to primary key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we make the final dictionary, we need to deal with date format. Django's SQLite expects `YYYY-MM-DD` format. So, we need to fix our date column a little bit. What you see is only string (a series of charactor). Python has a date (or datetime) variable for dealing with this type of data. Date data type is numerial. So, we can do calulation and also, in our case, format in the way we want. First, we need to convert string to datetime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have a date column which we can do many thing on it. But we will just convert it back into string (with the format that we want). Then we will make a dictionary and JSON. The function we use is `strftime` (string from time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have everything ready. Let's create the final dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = []\n",
    "for index, row in sub_log_df.iterrows():\n",
    "    logs.append({\n",
    "        \"model\": \"acrelog.event\",\n",
    "        \"pk\": index+1,\n",
    "        \"fields\": {\n",
    "            # \"primary_key\": row['Event#'],\n",
    "            \"date\": row['date'],\n",
    "            \"event\": row['operation'],\n",
    "            \"note\": row['note'],\n",
    "        }\n",
    "    })\n",
    "logs\n",
    "\n",
    "json.dumps(logs)\n",
    "# or write the JSON to the file directly \n",
    "with open('event-data.json', 'w') as fp:\n",
    "    json.dump(logs, fp=fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can dump the dictionary to JSON then load the JSON file to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list which we will add dictionaries later\n",
    "operator_list = [] # suffix s is common way to indicate a list (array)\n",
    "\n",
    "# ! check this DK\n",
    "for indv in log_df['operator'].unique(): # we iterate over the result from unique function\n",
    "    location_list.append( # append new element to the list\n",
    "        { # curly bracket indicate the starting of dictionary\n",
    "            \"location\": indv, # each key-value pair\n",
    "        } # the end of dictionary\n",
    "    )\n",
    "\n",
    "location_df = pd.DataFrame(location_list)\n",
    "\n",
    "locations = [] # create an empty list\n",
    "for index, row in location_df.iterrows(): # we iterate over the dataframe row by row\n",
    "    locations.append( # append new element to the list\n",
    "        { # curly bracket indicate the starting of dictionary\n",
    "            \"model\": \"acrelog.location\",  # the class name\n",
    "            \"pk\": index+1, # index starts at 0 while primary key starts at 1. \n",
    "            \"fields\": { # this is a dictionary of attributes\n",
    "                \"location_name\": row['location'] # only need firstname for now\n",
    "            }\n",
    "        } # the end of dictionary\n",
    "    )\n",
    "\n",
    "json.dumps(locations)\n",
    "# or write the JSON to the file directly \n",
    "with open('location-data.json', 'w') as fp:\n",
    "    json.dump(locations, fp=fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list which we will add dictionaries later\n",
    "operator_list = [] # suffix s is common way to indicate a list (array)\n",
    "\n",
    "# ! check this DK\n",
    "for indv in log_df['operator'].unique(): # we iterate over the result from unique function\n",
    "    operator_list.append( # append new element to the list\n",
    "        { # curly bracket indicate the starting of dictionary\n",
    "            \"operator\": indv, # each key-value pair\n",
    "        } # the end of dictionary\n",
    "    )\n",
    "\n",
    "operator_df = pd.DataFrame(operator_list)\n",
    "\n",
    "operators = [] # create an empty list\n",
    "for index, row in operator_df.iterrows(): # we iterate over the dataframe row by row\n",
    "    operators.append( # append new element to the list\n",
    "        { # curly bracket indicate the starting of dictionary\n",
    "            \"model\": \"acrelog.operator\",  # the class name\n",
    "            \"pk\": index+1, # index starts at 0 while primary key starts at 1. \n",
    "            \"fields\": { # this is a dictionary of attributes\n",
    "                \"operator_name\": row['operator'] # only need firstname for now\n",
    "            }\n",
    "        } # the end of dictionary\n",
    "    )\n",
    "\n",
    "json.dumps(operators)\n",
    "# or write the JSON to the file directly \n",
    "with open('operator-data.json', 'w') as fp:\n",
    "    json.dump(operators, fp=fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list which we will add dictionaries later\n",
    "Power_Unit_list = [] # suffix s is common way to indicate a list (array)\n",
    "\n",
    "# ! check this DK\n",
    "for indv in log_df['Power Unit'].unique(): # we iterate over the result from unique function\n",
    "    Power_Unit_list.append( # append new element to the list\n",
    "        { # curly bracket indicate the starting of dictionary\n",
    "            \"Power Unit\": indv, # each key-value pair\n",
    "        } # the end of dictionary\n",
    "    )\n",
    "\n",
    "Power_Unit_df = pd.DataFrame(Power_Unit_list)\n",
    "\n",
    "Power_Units = [] # create an empty list\n",
    "for index, row in Power_Unit_df.iterrows(): # we iterate over the dataframe row by row\n",
    "    Power_Units.append( # append new element to the list\n",
    "        { # curly bracket indicate the starting of dictionary\n",
    "            \"model\": \"acrelog.spray\",  # the class name\n",
    "            \"pk\": index+1, # index starts at 0 while primary key starts at 1. \n",
    "            \"fields\": { # this is a dictionary of attributes\n",
    "                \"powerunit_type\": row['Power Unit'] # only need firstname for now\n",
    "            }\n",
    "        } # the end of dictionary\n",
    "    )\n",
    "\n",
    "json.dumps(Power_Units)\n",
    "# or write the JSON to the file directly \n",
    "with open('Power_Unit-data.json', 'w') as fp:\n",
    "    json.dump(Power_Units, fp=fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hagie STS12', 1], [nan, 2], ['4555', 3], ['8245R', 4], ['4240', 5], ['8R370', 6], ['Not applicable', 7], ['Case Magnum', 8], ['4440', 9], ['Gator', 10], ['6400 Cab', 11], ['S660', 12], ['8R370X', 13]]\n",
      "{'Hagie STS12': 1, nan: 2, '4555': 3, '8245R': 4, '4240': 5, '8R370': 6, 'Not applicable': 7, 'Case Magnum': 8, '4440': 9, 'Gator': 10, '6400 Cab': 11, 'S660': 12, '8R370X': 13}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "powerunits = []\n",
    "# ! check this DK\n",
    "for powerunit in log_df['Power Unit'].unique(): # we iterate over the result from unique function\n",
    "    powerunits.append( # append new element to the list\n",
    "        { # curly bracket indicate the starting of dictionary\n",
    "            \"powerunit_type\": powerunit, # each key-value pair\n",
    "        } # the end of dictionary\n",
    "    )\n",
    "\n",
    "for k in range(len(powerunits)):\n",
    "    powerunits[k] = [powerunits[k].get('powerunit_type'), k+1]\n",
    "print(powerunits)\n",
    "# convert the list to a dict\n",
    "powerunits = dict(powerunits)\n",
    "print(powerunits)\n",
    "\n",
    "\n",
    "\n",
    "# you can subset the dataframe with any columns (and order)\n",
    "sub_log_df = log_df[['Seeds planted', 'Seeding Rate (seeds/ac)']]\n",
    "# if whole row is 'NaN', drop the row\n",
    "sub_log_df = sub_log_df.dropna(how='all')\n",
    "# replace 'NaN' with 0\n",
    "sub_log_df = sub_log_df.fillna(0)\n",
    "\n",
    "# add a new column 'Event#' to the event_df\n",
    "# sub_log_df['Event#'] = sub_log_df.index + 1\n",
    "# pd.to_datetime(sub_log_df['date'])\n",
    "# sub_log_df['date'] = pd.to_datetime(sub_log_df['date']).dt.strftime('%Y-%m-%d')\n",
    "sub_log_df\n",
    "# drop duplicates\n",
    "sub_log_df = sub_log_df.drop_duplicates()\n",
    "\n",
    "logs = []\n",
    "for index, row in sub_log_df.iterrows():\n",
    "    logs.append({\n",
    "        \"model\": \"acrelog.seed\",\n",
    "        \"pk\": index+1,\n",
    "        \"fields\": {\n",
    "            \"seed_planted\": row['Seeds planted'],\n",
    "            \"seed_rate\": row['Seeding Rate (seeds/ac)'],\n",
    "        }\n",
    "    })\n",
    "logs\n",
    "\n",
    "json.dumps(logs)\n",
    "# or write the JSON to the file directly \n",
    "with open('seed-data.json', 'w') as fp:\n",
    "    json.dump(logs, fp=fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan, 1], ['ATS', 2], ['28% UAN ', 3], ['10-34-0', 4], ['10-34-0,28% UAN ,Other - refer to notes', 5], ['28% UAN ,ATS', 6], ['MAP,Potash (0-0-60)', 7], ['Potash (0-0-60)', 8], ['Potash (0-0-60),MAP', 9], ['Lime', 10], ['MAP', 11]]\n",
      "{nan: 1, 'ATS': 2, '28% UAN ': 3, '10-34-0': 4, '10-34-0,28% UAN ,Other - refer to notes': 5, '28% UAN ,ATS': 6, 'MAP,Potash (0-0-60)': 7, 'Potash (0-0-60)': 8, 'Potash (0-0-60),MAP': 9, 'Lime': 10, 'MAP': 11}\n"
     ]
    }
   ],
   "source": [
    "fertilizers = []\n",
    "# ! check this DK\n",
    "for fertilizer in log_df['Fertilizers applied'].unique(): # we iterate over the result from unique function\n",
    "    fertilizers.append( # append new element to the list\n",
    "        { # curly bracket indicate the starting of dictionary\n",
    "            \"fertilizer\": fertilizer, # each key-value pair\n",
    "        } # the end of dictionary\n",
    "    )\n",
    "\n",
    "for k in range(len(fertilizers)):\n",
    "    fertilizers[k] = [fertilizers[k].get('fertilizer'), k+1]\n",
    "print(fertilizers)\n",
    "# convert the list to a dict\n",
    "fertilizers = dict(fertilizers)\n",
    "print(fertilizers)\n",
    "\n",
    "\n",
    "\n",
    "# you can subset the dataframe with any columns (and order)\n",
    "sub_log_df = log_df[['Fertilizers applied', 'Fertilizer Rate (lb/ac)']]\n",
    "# if whole row is 'NaN', drop the row\n",
    "sub_log_df = sub_log_df.dropna(how='all')\n",
    "# replace 'NaN' with 0\n",
    "sub_log_df = sub_log_df.fillna(0)\n",
    "\n",
    "# add a new column 'Event#' to the event_df\n",
    "# sub_log_df['Event#'] = sub_log_df.index + 1\n",
    "# pd.to_datetime(sub_log_df['date'])\n",
    "# sub_log_df['date'] = pd.to_datetime(sub_log_df['date']).dt.strftime('%Y-%m-%d')\n",
    "sub_log_df\n",
    "# drop duplicates\n",
    "sub_log_df = sub_log_df.drop_duplicates()\n",
    "\n",
    "logs = []\n",
    "for index, row in sub_log_df.iterrows():\n",
    "    logs.append({\n",
    "        \"model\": \"acrelog.fertilizer\",\n",
    "        \"pk\": index+1,\n",
    "        \"fields\": {\n",
    "            \"fertilizer_type\": row['Fertilizers applied'],\n",
    "            \"fertilizer_rate\": row['Fertilizer Rate (lb/ac)'],\n",
    "        }\n",
    "    })\n",
    "logs\n",
    "\n",
    "json.dumps(logs)\n",
    "# or write the JSON to the file directly \n",
    "with open('fertilizer-data.json', 'w') as fp:\n",
    "    json.dump(logs, fp=fp, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asm591",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
